{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "sns.set_context(\"paper\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "height_data = pd.read_csv('Galton.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "Neural networks are all the rage nowadays: the solution to all our problems seem to be to add more layers. This article takes a breath of air away from the hype, and returns to investigate it's roots: linear  regression. Linear regression is a very expansive subject, and many statistics majors can spend their entire undergraduate career within the field. We'll try to do the subject justice (read: this is a long article).\n",
    "\n",
    "\n",
    "#### Part 1: Linear Regression through Statistical Decision Theory\n",
    "\n",
    "1. An Intro to Decision Theory\n",
    "2. Empirical Risk Minimization (ERM)\n",
    "3. ERM with Linear Functions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression through Statistical Decision Theory\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## An Intro to Decision Theory\n",
    "\n",
    "**Regression**, as a subset of *supervised learning*, is the task of predicting continuous-valued outputs from data. We'd like to find an *estimator* $\\delta(X)$ that *best* predicts some output $Y$ best from input $X$. To formalize the concept of a \"best\" estimator, we need to borrow some terms from statistical decision theory.\n",
    "\n",
    "The true connection between the random variables $X \\in \\mathbb{R}^m$ (our input variable) and $Y \\in \\mathbb{R}$ (our output variable) as a joint  probability distribution $\\mathcal{P}(X,Y)$. By writing it stochastically,we capture inherent randomness in the connection between the two variables, and errors in measurement. Machine learning, in a nutshell, is the process of creating an estimator $\\delta$ from observed training data $X_{train},Y_{train} \\sim \\mathcal{P}$\n",
    "\n",
    "An estimator $\\delta(X)$ of $Y$ incurs a **loss** $L(\\delta(X),Y)$, which measures the deviation between the predicted value and the true value. For example, common losses include\n",
    "\n",
    "- Squared Error Loss: $L(\\delta(X),Y) = (\\delta(X)-Y)^2$\n",
    "- Absolute Deviation Loss: $L(\\delta(X),Y) = |\\delta(X)-Y|$\n",
    "\n",
    "Choice of a loss induces the **risk** of an estimator, \n",
    "\n",
    "$$R(\\delta) = \\mathbb{E}_{X,Y \\sim \\mathcal{P}}[L(\\delta(X),Y)]$$\n",
    "\n",
    "Risk can be thought of as the \"expected\" deviation between the predicted value and the true value of the output. The ideal estimator minimizes the risk, although since we do not know the true probability distribution $\\mathcal{P}$, computing this risk is impossible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Empirical Risk Minimization\n",
    "\n",
    "Since computing the true risk is impossible, we consider the **empirical risk** of an estimator instead. This is given by \n",
    "$$R_{emp}(\\delta) = \\frac1n \\sum_{i=1}^n L(\\delta(X_i),Y_i))$$\n",
    "\n",
    "Here, we use the notation $X_{train} = (X_i)_{i=1}^n$,$Y_{train} = (Y_i)_{i=1}^n$.\n",
    "\n",
    "\n",
    "The empirical risk is the average loss on the training data, and can also be interpreted as the true risk under the *empirical distribution*. \n",
    "\n",
    "**Empirical Risk Minimization** is the procedure which finds an estimator $\\delta_{emp}$ which minimizes the empirical risk as a surrogate for the true risk.\n",
    "$$\\delta_{emp} = \\arg \\min_{\\delta} R_{emp}(\\delta)$$\n",
    "\n",
    "Intuitively, this is a sensible idea, because we expect that if we have a lot of data, then the empirical risk of an estimator is roughly equal to the true risk. \n",
    "\n",
    "**Important Note:** The empirical risk may be very different than the true risk if the distribution of $X_{train},Y_{train}$ is significantly different than the true distribution. This is a very common real-world experience, especially if there was sampling bias in collecting the data.\n",
    "\n",
    "We can rewrite the true risk in a way to reveal this structure\n",
    "$$\n",
    "R(\\delta) = \\underbrace{R(\\delta) - R_{emp}(\\delta)}_{\\text{Generalization Error}} + \\underbrace{R_{emp}(\\delta)}_{\\text{Empirical Risk}}\n",
    "$$\n",
    "\n",
    "In order to minimize the true risk, we need to balance between the generalization error and the empirical risk. If a estimator minimizes the empirical risk fully, it may be overfitting to the training data, and exhibit poor generalization. Alternately, one could have an estimator which is equally poor on the true and empirical distributions (and thus have great generalization), but still have very poor loss. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Empirical Risk Minimization with Linear Functions\n",
    "\n",
    "Consider finding the estimator $\\delta_{erm}$ which minimizes the empirical risk amongst all linear functions of the data. In other words, we find the best estimator of form\n",
    "\n",
    "$$\\delta(x) =w^Tx~~~~~w \\in \\mathbb{R}^m$$\n",
    "\n",
    "Finding the optimal $\\delta$ can be reduced into finding the optimal $w \\in \\mathbb{R}^m$ which minimizes the empirical risk objective. Note however that the optimal weight vector $w^*$ may not be unique: we'll talk more about this later.\n",
    "\n",
    "**Empirical Risk Objective** $$w^* = \\arg \\min_{w \\in \\mathbb{R}^m} \\frac{1}{n} \\sum_{i=1}^n (w^TX_i - Y_i)^2$$\n",
    "\n",
    "If we define $X = \\begin{bmatrix} --X_1-- \\\\ \\vdots \\\\ --X_n--\\end{bmatrix}$ as a $n \\times m$ matrix where each row is a data point, and $Y =  \\begin{bmatrix} Y_1 \\\\ \\vdots \\\\ Y_n\\end{bmatrix}$ a vector in $\\mathbb{R}^n$, then this objective can be written in vector notation as \n",
    "\n",
    "$$w^* = \\arg \\min_{w \\in \\mathbb{R}^m} \\frac{1}{n} \\|Xw-Y\\|_2^2 = \\arg \\min_{w \\in \\mathbb{R}^m} \\|Xw-Y\\|_2^2$$\n",
    "\n",
    "< TO DO FILL IN >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Expected Risks of Estimators\n",
    "\n",
    "When the loss metric is the *squared error loss*, we can consider the \"expected risk\" of an estimator over different training sets. Recall that we define the bias of an estimator to be \n",
    "\n",
    "$$\\text{Bias}(\\delta(X)) = \\mathbb{E}_{train}[\\delta(X)-Y]$$\n",
    "where $\\mathbb{E}_{train} = \\mathbb{E}_{X_{train},Y_{train} \\sim \\mathcal{P}}$ refers to the expectation over different training data. \n",
    "\n",
    "For convenience sake, we define $\\overline{\\delta(X)} = \\mathbb{E}_{train}[\\delta(X)]$. \n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\mathbb{E}_{train}[R(\\delta)]\n",
    " &= \\mathbb{E}_{train}[\\mathbb{E}_{X,Y \\sim \\mathcal{P}}[L(\\delta_{t}(X,Y))]]\\\\\n",
    " &=  \\mathbb{E}_{X,Y \\sim \\mathcal{P}}[\\mathbb{E}_{train}[L(\\delta_{t}(X,Y))]]\\\\\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "\n",
    "Thus, for a general $X$,$Y$\n",
    "\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\mathbb{E}_{train}[L(\\delta(X,Y))]\n",
    "&= \\mathbb{E}_{train}[(\\delta(X)-Y)^2]\\\\\n",
    "&= \\mathbb{E}_{train}[(\\delta(X)- \\overline{\\delta(X)}  + \\overline{\\delta(X)} - Y)^2]\\\\\n",
    "&= \\mathbb{E}_{train}\\left[(\\delta(X)-\\overline{\\delta(X)} )^2\\right] + \\mathbb{E}_{train}\\left[(\\overline{\\delta(X)}  - Y)^2\\right] + 2\\mathbb{E}_{train}\\left[(\\delta(X)-\\overline{\\delta(X)} )(\\overline{\\delta(X)}  - Y)\\right]\\\\\n",
    "&= \\mathbb{E}_{train}\\left[(\\delta(X)-\\overline{\\delta(X)} )^2\\right] + (\\overline{\\delta(X)}  - Y)^2 + 2(\\overline{\\delta(X)}  - Y)\\mathbb{E}_{train}\\left[(\\delta(X)-\\overline{\\delta(X)} )\\right]\\\\\n",
    "&= Var(\\delta(X)) + Bias(\\delta(X))^2\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "The expected loss over the training set is a function of the *variance* of the estimator (how much does the estimator change over different training sets? ), and the *bias* of the estimator: (on average, how far will the estimator be?) This fundamental decomposition is called the **bias-variance tradeoff**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finally, Linear Regression\n",
    "\n",
    "Linear regression considers finding the optimal linear estimator with  $\\delta(X) = w^TX$. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
